{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPogVky3n90n",
        "outputId": "140d14cb-e45d-4c66-816c-a9bd6ca2b5e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyarrow==14.0.1\n",
            "  Downloading pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (38.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow==14.0.1) (1.25.2)\n",
            "Installing collected packages: pyarrow\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "Successfully installed pyarrow-14.0.1\n",
            "Collecting llama-index\n",
            "  Downloading llama_index-0.10.52-py3-none-any.whl (6.8 kB)\n",
            "Collecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.2.7-py3-none-any.whl (12 kB)\n",
            "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_cli-0.1.12-py3-none-any.whl (26 kB)\n",
            "Collecting llama-index-core==0.10.52.post1 (from llama-index)\n",
            "  Downloading llama_index_core-0.10.52.post1-py3-none-any.whl (15.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.1.10-py3-none-any.whl (6.2 kB)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.2.3-py3-none-any.whl (9.2 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
            "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-llms-openai<0.2.0,>=0.1.13 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.1.25-py3-none-any.whl (11 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.1.7-py3-none-any.whl (5.9 kB)\n",
            "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.1.6-py3-none-any.whl (5.2 kB)\n",
            "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
            "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.1.27-py3-none-any.whl (37 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.1.6-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (3.9.5)\n",
            "Collecting dataclasses-json (from llama-index-core==0.10.52.post1->llama-index)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core==0.10.52.post1->llama-index)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core==0.10.52.post1->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (2023.6.0)\n",
            "Collecting httpx (from llama-index-core==0.10.52.post1->llama-index)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-cloud<0.0.7,>=0.0.6 (from llama-index-core==0.10.52.post1->llama-index)\n",
            "  Downloading llama_cloud-0.0.6-py3-none-any.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.8/130.8 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (3.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (1.25.2)\n",
            "Collecting openai>=1.1.0 (from llama-index-core==0.10.52.post1->llama-index)\n",
            "  Downloading openai-1.35.10-py3-none-any.whl (328 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.3/328.3 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (8.4.2)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core==0.10.52.post1->llama-index)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (4.12.2)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core==0.10.52.post1->llama-index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (1.14.1)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.12.3)\n",
            "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index)\n",
            "  Downloading llama_parse-0.4.5-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.52.post1->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.52.post1->llama-index) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.52.post1->llama-index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.52.post1->llama-index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.52.post1->llama-index) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.52.post1->llama-index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (2.5)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llama-cloud<0.0.7,>=0.0.6->llama-index-core==0.10.52.post1->llama-index) (2.8.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.52.post1->llama-index) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.52.post1->llama-index) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx->llama-index-core==0.10.52.post1->llama-index)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.52.post1->llama-index) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.52.post1->llama-index) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core==0.10.52.post1->llama-index)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.52.post1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.52.post1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.52.post1->llama-index) (2024.5.15)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core==0.10.52.post1->llama-index) (1.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core==0.10.52.post1->llama-index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core==0.10.52.post1->llama-index) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.52.post1->llama-index) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core==0.10.52.post1->llama-index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core==0.10.52.post1->llama-index)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core==0.10.52.post1->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core==0.10.52.post1->llama-index) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core==0.10.52.post1->llama-index) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core==0.10.52.post1->llama-index) (1.2.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core==0.10.52.post1->llama-index) (24.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llama-cloud<0.0.7,>=0.0.6->llama-index-core==0.10.52.post1->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llama-cloud<0.0.7,>=0.0.6->llama-index-core==0.10.52.post1->llama-index) (2.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core==0.10.52.post1->llama-index) (1.16.0)\n",
            "Installing collected packages: striprtf, dirtyjson, pypdf, mypy-extensions, marshmallow, h11, deprecated, typing-inspect, tiktoken, httpcore, httpx, dataclasses-json, openai, llama-cloud, llama-index-legacy, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "Successfully installed dataclasses-json-0.6.7 deprecated-1.2.14 dirtyjson-1.0.8 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 llama-cloud-0.0.6 llama-index-0.10.52 llama-index-agent-openai-0.2.7 llama-index-cli-0.1.12 llama-index-core-0.10.52.post1 llama-index-embeddings-openai-0.1.10 llama-index-indices-managed-llama-cloud-0.2.3 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.25 llama-index-multi-modal-llms-openai-0.1.7 llama-index-program-openai-0.1.6 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.27 llama-index-readers-llama-parse-0.1.6 llama-parse-0.4.5 marshmallow-3.21.3 mypy-extensions-1.0.0 openai-1.35.10 pypdf-4.2.0 striprtf-0.0.26 tiktoken-0.7.0 typing-inspect-0.9.0\n",
            "Collecting ragas\n",
            "  Downloading ragas-0.1.10-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.5/91.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ragas) (1.25.2)\n",
            "Collecting datasets (from ragas)\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from ragas) (0.7.0)\n",
            "Collecting langchain (from ragas)\n",
            "  Downloading langchain-0.2.6-py3-none-any.whl (975 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m975.5/975.5 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core (from ragas)\n",
            "  Downloading langchain_core-0.2.11-py3-none-any.whl (337 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.4/337.4 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-community (from ragas)\n",
            "  Downloading langchain_community-0.2.6-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-openai (from ragas)\n",
            "  Downloading langchain_openai-0.1.14-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: openai>1 in /usr/local/lib/python3.10/dist-packages (from ragas) (1.35.10)\n",
            "Collecting pysbd>=0.3.4 (from ragas)\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ragas) (1.6.0)\n",
            "Collecting appdirs (from ragas)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>1->ragas) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (2.8.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (3.15.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets->ragas)\n",
            "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->ragas)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (2.0.3)\n",
            "Collecting requests>=2.32.2 (from datasets->ragas)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets->ragas)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets->ragas)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (0.23.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->ragas) (2.0.31)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain->ragas) (4.0.3)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain->ragas)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain->ragas)\n",
            "  Downloading langsmith-0.1.83-py3-none-any.whl (127 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.5/127.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->ragas) (8.4.2)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core->ragas)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community->ragas) (0.6.7)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->ragas) (2024.5.15)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>1->ragas) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>1->ragas) (1.2.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>1->ragas) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>1->ragas) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas) (0.14.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core->ragas)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain->ragas)\n",
            "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>1->ragas) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>1->ragas) (2.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->ragas) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->ragas) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->ragas) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas) (1.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (1.0.0)\n",
            "Installing collected packages: appdirs, xxhash, requests, pysbd, pyarrow, orjson, jsonpointer, dill, multiprocess, jsonpatch, langsmith, langchain-core, datasets, langchain-text-splitters, langchain-openai, langchain, langchain-community, ragas\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.1\n",
            "    Uninstalling pyarrow-14.0.1:\n",
            "      Successfully uninstalled pyarrow-14.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed appdirs-1.4.4 datasets-2.20.0 dill-0.3.8 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.6 langchain-community-0.2.6 langchain-core-0.2.11 langchain-openai-0.1.14 langchain-text-splitters-0.2.2 langsmith-0.1.83 multiprocess-0.70.16 orjson-3.10.6 pyarrow-16.1.0 pysbd-0.3.4 ragas-0.1.10 requests-2.32.3 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyarrow==14.0.1   # Replace with a compatible version\n",
        "!pip install llama-index\n",
        "!pip install ragas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Sh_KQn0n86_",
        "outputId": "cd013fc2-aacb-4aab-aa95-880cf7cc20fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index-llms-anyscale\n",
            "  Downloading llama_index_llms_anyscale-0.1.4-py3-none-any.whl (4.2 kB)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-anyscale) (0.10.52.post1)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-anyscale) (0.1.25)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (0.27.0)\n",
            "Requirement already satisfied: llama-cloud<0.0.7,>=0.0.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (0.0.6)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (3.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (1.25.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (1.35.10)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (8.4.2)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (1.14.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (4.0.3)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llama-cloud<0.0.7,>=0.0.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (2.8.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (2024.5.15)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (1.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (3.21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (1.2.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (24.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llama-cloud<0.0.7,>=0.0.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llama-cloud<0.0.7,>=0.0.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (2.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anyscale) (1.16.0)\n",
            "Installing collected packages: llama-index-llms-anyscale\n",
            "Successfully installed llama-index-llms-anyscale-0.1.4\n"
          ]
        }
      ],
      "source": [
        "%pip install llama-index-llms-anyscale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gQrQNIKoV6K",
        "outputId": "adc3cced-a279-4a9b-d932-c49d3a52c96e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index-embeddings-anyscale\n",
            "  Downloading llama_index_embeddings_anyscale-0.1.2-py3-none-any.whl (4.4 kB)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-embeddings-anyscale) (0.10.52.post1)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-embeddings-anyscale) (0.1.25)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (0.27.0)\n",
            "Requirement already satisfied: llama-cloud<0.0.7,>=0.0.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (0.0.6)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (3.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (1.25.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (1.35.10)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (8.4.2)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (1.14.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (4.0.3)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llama-cloud<0.0.7,>=0.0.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (2.8.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (2024.5.15)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (1.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (3.21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (1.2.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (24.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llama-cloud<0.0.7,>=0.0.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llama-cloud<0.0.7,>=0.0.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (2.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-anyscale) (1.16.0)\n",
            "Installing collected packages: llama-index-embeddings-anyscale\n",
            "Successfully installed llama-index-embeddings-anyscale-0.1.2\n"
          ]
        }
      ],
      "source": [
        "%pip install llama-index-embeddings-anyscale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WeT7FMPoBkU",
        "outputId": "353e3af0-240c-4beb-8363-316524eb3138"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Anyscale\n",
            "  Downloading anyscale-0.24.47.tar.gz (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting boto3>=1.26.76 (from Anyscale)\n",
            "  Downloading boto3-1.34.140-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore>=1.19.52 (from Anyscale)\n",
            "  Downloading botocore-1.34.140-py3-none-any.whl (12.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp>=3.7.4.post0 in /usr/local/lib/python3.10/dist-packages (from Anyscale) (3.9.5)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from Anyscale) (2024.6.2)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.10/dist-packages (from Anyscale) (8.1.7)\n",
            "Collecting colorama (from Anyscale)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting GitPython (from Anyscale)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (from Anyscale) (2.27.0)\n",
            "Requirement already satisfied: jsonpatch in /usr/local/lib/python3.10/dist-packages (from Anyscale) (1.33)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from Anyscale) (4.19.2)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from Anyscale) (4.1.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from Anyscale) (24.1)\n",
            "Collecting pathspec>=0.8.1 (from Anyscale)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from Anyscale) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from Anyscale) (2.32.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from Anyscale) (13.7.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from Anyscale) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from Anyscale) (0.9.0)\n",
            "Requirement already satisfied: urllib3>=1.26.17 in /usr/local/lib/python3.10/dist-packages (from Anyscale) (2.0.7)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from Anyscale) (1.14.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from Anyscale) (6.0.1)\n",
            "Requirement already satisfied: smart_open in /usr/local/lib/python3.10/dist-packages (from Anyscale) (7.0.4)\n",
            "Collecting halo (from Anyscale)\n",
            "  Downloading halo-0.0.31.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from Anyscale) (4.66.4)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from Anyscale) (5.2)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (from Anyscale) (4.7.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from Anyscale) (4.12.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7.4.post0->Anyscale) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7.4.post0->Anyscale) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7.4.post0->Anyscale) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7.4.post0->Anyscale) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7.4.post0->Anyscale) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7.4.post0->Anyscale) (4.0.3)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.26.76->Anyscale)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.26.76->Anyscale)\n",
            "  Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1 (from GitPython->Anyscale)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->Anyscale) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth->Anyscale) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth->Anyscale) (4.9)\n",
            "Collecting log_symbols>=0.0.14 (from halo->Anyscale)\n",
            "  Downloading log_symbols-0.0.14-py3-none-any.whl (3.1 kB)\n",
            "Collecting spinners>=0.0.24 (from halo->Anyscale)\n",
            "  Downloading spinners-0.0.24-py3-none-any.whl (5.5 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from halo->Anyscale) (2.4.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch->Anyscale) (3.0.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->Anyscale) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->Anyscale) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->Anyscale) (0.18.1)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from oauth2client->Anyscale) (0.22.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client->Anyscale) (0.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->Anyscale) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->Anyscale) (3.7)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->Anyscale) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->Anyscale) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython->Anyscale)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2>=0.9.1->oauth2client->Anyscale) (3.1.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->Anyscale) (0.1.2)\n",
            "Building wheels for collected packages: Anyscale, halo\n",
            "  Building wheel for Anyscale (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Anyscale: filename=anyscale-0.24.47-cp310-cp310-linux_x86_64.whl size=2426795 sha256=93d9ca9897fb82a92a93bea444aab44cf6d36323162e8c7f80b865cc6031abac\n",
            "  Stored in directory: /root/.cache/pip/wheels/d4/e2/e6/73deeab7e6a26b3a230c68fd831ab90e8baee62392eda3d15c\n",
            "  Building wheel for halo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for halo: filename=halo-0.0.31-py3-none-any.whl size=11232 sha256=a132a8efc2750b89e6097e0088c15e1f653a8eabe4c7cbf08029cdf0fb7d7efb\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/d9/8a/b4f14c44aba7c164d4379eca6f1dde59360050406b1edaec24\n",
            "Successfully built Anyscale halo\n",
            "Installing collected packages: spinners, smmap, pathspec, jmespath, colorama, log_symbols, gitdb, botocore, s3transfer, halo, GitPython, boto3, Anyscale\n",
            "Successfully installed Anyscale-0.24.47 GitPython-3.1.43 boto3-1.34.140 botocore-1.34.140 colorama-0.4.6 gitdb-4.0.11 halo-0.0.31 jmespath-1.0.1 log_symbols-0.0.14 pathspec-0.12.1 s3transfer-0.10.2 smmap-5.0.1 spinners-0.0.24\n"
          ]
        }
      ],
      "source": [
        "!pip install Anyscale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpdAdQLRn90r"
      },
      "outputs": [],
      "source": [
        "# The nest_asyncio module enables the nesting of asynchronous functions within an already running async loop.\n",
        "# This is necessary because Jupyter notebooks inherently operate in an asynchronous loop.\n",
        "# By applying nest_asyncio, we can run additional async functions within this existing loop without conflicts.\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from llama_index.core.evaluation import generate_question_context_pairs\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
        "from llama_index.core.node_parser import SimpleNodeParser\n",
        "from llama_index.core.evaluation import RetrieverEvaluator\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "import os\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiaEbWzFn90s",
        "outputId": "71840566-a431-4f02-aa37-856757fbd844"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.35.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAh2xlbfsmlj"
      },
      "outputs": [],
      "source": [
        "os.environ['OPENAI_API_KEY'] = ''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmgDzKGcn90s",
        "outputId": "d400a44f-4bd9-4a99-a44b-b0e1a485891e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/bin/python3: Error while finding module specification for 'vllm.entrypoints.openai.api_server' (ModuleNotFoundError: No module named 'vllm')\n"
          ]
        }
      ],
      "source": [
        "# start the vLLM server\n",
        "!python -m vllm.entrypoints.openai.api_server \\\n",
        "    --model HuggingFaceH4/zephyr-7b-alpha \\\n",
        "    --host 0.0.0.0 \\\n",
        "    --port 8080"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n10qMVz4n90u",
        "outputId": "d0f947b0-1662-4f5b-8885-38c972967d83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: anyscale in /usr/local/lib/python3.10/dist-packages (0.24.47)\n",
            "Requirement already satisfied: boto3>=1.26.76 in /usr/local/lib/python3.10/dist-packages (from anyscale) (1.34.140)\n",
            "Requirement already satisfied: botocore>=1.19.52 in /usr/local/lib/python3.10/dist-packages (from anyscale) (1.34.140)\n",
            "Requirement already satisfied: aiohttp>=3.7.4.post0 in /usr/local/lib/python3.10/dist-packages (from anyscale) (3.9.5)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from anyscale) (2024.6.2)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.10/dist-packages (from anyscale) (8.1.7)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from anyscale) (0.4.6)\n",
            "Requirement already satisfied: GitPython in /usr/local/lib/python3.10/dist-packages (from anyscale) (3.1.43)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (from anyscale) (2.27.0)\n",
            "Requirement already satisfied: jsonpatch in /usr/local/lib/python3.10/dist-packages (from anyscale) (1.33)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from anyscale) (4.19.2)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from anyscale) (4.1.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from anyscale) (24.1)\n",
            "Requirement already satisfied: pathspec>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from anyscale) (0.12.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from anyscale) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from anyscale) (2.32.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from anyscale) (13.7.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from anyscale) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from anyscale) (0.9.0)\n",
            "Requirement already satisfied: urllib3>=1.26.17 in /usr/local/lib/python3.10/dist-packages (from anyscale) (2.0.7)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from anyscale) (1.14.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from anyscale) (6.0.1)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.10/dist-packages (from anyscale) (7.0.4)\n",
            "Requirement already satisfied: halo in /usr/local/lib/python3.10/dist-packages (from anyscale) (0.0.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from anyscale) (4.66.4)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from anyscale) (5.2)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (from anyscale) (4.7.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from anyscale) (4.12.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7.4.post0->anyscale) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7.4.post0->anyscale) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7.4.post0->anyscale) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7.4.post0->anyscale) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7.4.post0->anyscale) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7.4.post0->anyscale) (4.0.3)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.26.76->anyscale) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.26.76->anyscale) (0.10.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython->anyscale) (4.0.11)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->anyscale) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth->anyscale) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth->anyscale) (4.9)\n",
            "Requirement already satisfied: log-symbols>=0.0.14 in /usr/local/lib/python3.10/dist-packages (from halo->anyscale) (0.0.14)\n",
            "Requirement already satisfied: spinners>=0.0.24 in /usr/local/lib/python3.10/dist-packages (from halo->anyscale) (0.0.24)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from halo->anyscale) (2.4.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch->anyscale) (3.0.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->anyscale) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->anyscale) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->anyscale) (0.18.1)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from oauth2client->anyscale) (0.22.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client->anyscale) (0.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->anyscale) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->anyscale) (3.7)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->anyscale) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->anyscale) (2.16.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython->anyscale) (5.0.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2>=0.9.1->oauth2client->anyscale) (3.1.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->anyscale) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install anyscale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2W0asPU4gAyY",
        "outputId": "135e4d46-ba62-47fd-e9c6-5a9e28c88610"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-index-readers-file in /usr/local/lib/python3.10/dist-packages (0.1.27)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file) (4.12.3)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.37.post1 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file) (0.10.52.post1)\n",
            "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file) (4.2.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file) (0.0.26)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file) (2.5)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (0.27.0)\n",
            "Requirement already satisfied: llama-cloud<0.0.7,>=0.0.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (0.0.6)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (3.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (1.25.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (1.35.10)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (8.4.2)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (1.14.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (4.0.3)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llama-cloud<0.0.7,>=0.0.6->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (2.8.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (2024.5.15)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (1.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (3.21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (1.2.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (24.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llama-cloud<0.0.7,>=0.0.6->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llama-cloud<0.0.7,>=0.0.6->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (2.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U llama-index-readers-file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pG__zR5kn90v",
        "outputId": "e1556927-541e-4b64-8192-3a9aa379bb5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 6 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 6 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 6 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from llama_index.llms.anyscale import Anyscale\n",
        "from llama_index.embeddings.anyscale import AnyscaleEmbedding\n",
        "from llama_index.core import VectorStoreIndex,ServiceContext\n",
        "\n",
        "documents = SimpleDirectoryReader(\"/content/Data\").load_data()\n",
        "ANYSCALE_ENDPOINT_TOKEN = \"\"\n",
        "# service_context = ServiceContext.from_defaults(\n",
        "#     llm=Anyscale(model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "#                  api_key=ANYSCALE_ENDPOINT_TOKEN),\n",
        "#     embed_model=AnyscaleEmbedding(model=\"thenlper/gte-large\",\n",
        "#                                   api_key=ANYSCALE_ENDPOINT_TOKEN),\n",
        "#     chunk_size=400\n",
        "\n",
        "from llama_index.core import Settings\n",
        "llm = Anyscale(model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "                 api_key=ANYSCALE_ENDPOINT_TOKEN)\n",
        "Settings.llm =Anyscale(model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "                 api_key=ANYSCALE_ENDPOINT_TOKEN)\n",
        "Settings.embed_model = AnyscaleEmbedding(model=\"thenlper/gte-large\",\n",
        "                                  api_key=ANYSCALE_ENDPOINT_TOKEN)\n",
        "Settings.chunk_size = 400"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sd_x1kU0n90w"
      },
      "outputs": [],
      "source": [
        "index = VectorStoreIndex.from_documents(documents)\n",
        "#query_engine = index.as_query_engine()\n",
        "query_engine = index.as_query_engine(similarity_top_k=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPgn6qv2tP9T"
      },
      "source": [
        "HyDE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCFaeLsttXEy"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
        "from llama_index.core.query_engine.transform_query_engine import (\n",
        "    TransformQueryEngine,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pX0FOPHjtMNT"
      },
      "outputs": [],
      "source": [
        "# HyDE setup\n",
        "hyde = HyDEQueryTransform(include_original=True)\n",
        "# Transform the query engine using HyDE\n",
        "hyde_query_engine = TransformQueryEngine(query_engine, hyde)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIGvGE4jfkbq"
      },
      "outputs": [],
      "source": [
        "response_vector = hyde_query_engine.query(\"Trường Đại học Khoa học tự nhiên, ĐHQG-HCM đào tạo ngành nào nhiều sinh viên nhất?, Trả lời bằng tiếng việt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "qRuepqJnqwDX",
        "outputId": "8ba5a128-1327-4853-da01-2f156e2b4119"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Trường Đại học Khoa học tự nhiên, ĐHQG-HCM đào tạo ngành Sinh học với số lượng sinh viên lớn nhất. Đối với năm học 2023 - 2024, quy mô đào tạo của ngành này là 180 sinh viên.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "response_vector.response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKs7IwRvq2ZH",
        "outputId": "72258120-27ef-432a-b128-13ca63dc8ee8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Unnamed: 0                                          questions  \\\n",
            "0            0  Nhóm ngành Toán của trường Đại học Khoa học tự...   \n",
            "1            1  Trường Đại học Khoa học tự nhiên, ĐHQG-HCM đào...   \n",
            "2            2  Ngành nào của trường Đại học Khoa học tự nhiên...   \n",
            "3            3  Trường Đại học Kinh tế - Luật, ĐHQG-HCM có bao...   \n",
            "4            4  Ngành Quản trị kinh doanh của trường Đại Học K...   \n",
            "5            5  Ngành Marketing của khoa Quản Trị Kinh Doanh t...   \n",
            "6            6  Ngành Hệ thống thông tin quản lý của khoa Hệ t...   \n",
            "7            7  Khoa Y, ĐHQG-HCM có đào tạo ngành Y học dự phò...   \n",
            "8            8  Khoa Sư phạm trường Đại học An Giang, ĐHQG-HCM...   \n",
            "9            9  Học phí trung bình đại trà trường Đại học Bách...   \n",
            "10          10  Quy mô đào tạo Ngành kỹ thuật dầu khí và địa c...   \n",
            "11          11  Các ngành đào tạo của khoa Du lịch trường Đại ...   \n",
            "12          12  Chỉ tiêu ngành Thương mại Điện tử chương trình...   \n",
            "13          13  Học phí các chương trình đào tạo của khoa Thươ...   \n",
            "14          14  Trường Đại học Khoa học xã hội và nhân văn, ĐH...   \n",
            "15          15  Kể tên các chương trình đào tạo của trường Đại...   \n",
            "16          16  Trong khối Đại học quốc gia Thành phố Hồ Chí M...   \n",
            "17          17  Trong khối Đại học quốc gia Thành phố Hồ Chí M...   \n",
            "\n",
            "                                         ground_truth  \n",
            "0   ['Nhóm ngành Toán bao gồm ngành Toán học, ngàn...  \n",
            "1   ['Ngành Công nghệ thông tin Chương trình Chất ...  \n",
            "2   ['Chuyên ngành Thị giác máy tính được đào tạo ...  \n",
            "3   ['Trường Đại học Kinh tế - Luật, ĐHQG-HCM có 9...  \n",
            "4   ['Ngành Quản trị kinh doanh của trường Đại Học...  \n",
            "5   ['Ngành Marketing của khoa Quản Trị Kinh Doanh...  \n",
            "6   ['Ngành Hệ thống thông tin quản lý của khoa Hệ...  \n",
            "7   ['Khoa Y, ĐHQG-HCM không đào tạo ngành Y học d...  \n",
            "8   ['Khoa Sư phạm trường Đại học An Giang, ĐHQG-H...  \n",
            "9   ['Học phí trung bình đại trà trường Đại học Bá...  \n",
            "10  ['Quy mô đào tạo Ngành kỹ thuật dầu khí và địa...  \n",
            "11  ['Khoa Du lịch trường Đại học Khoa học xã hội ...  \n",
            "12  ['Chỉ tiêu ngành Thương mại Điện tử chương trì...  \n",
            "13  ['Học phí các chương trình đào tạo của khoa Th...  \n",
            "14  ['Trường Đại học Khoa học xã hội và nhân văn, ...  \n",
            "15  ['Các chương trình đào tạo của trường Đại học ...  \n",
            "16  ['Các trường thuộc khối Đại học quốc gia Thành...  \n",
            "17  ['Trong khối Đại học quốc gia Thành phố Hồ Chí...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Specify the file path\n",
        "file_path = r'/content/testtt.xlsx'\n",
        "\n",
        "# Read the Excel file into a DataFrame\n",
        "test = pd.read_excel(file_path)\n",
        "\n",
        "# Print the DataFrame\n",
        "print(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zSVe2GVrEOu"
      },
      "outputs": [],
      "source": [
        "questions = test[\"questions\"].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337,
          "referenced_widgets": [
            "cc99d0197f26458e821e463b17bfc9e1",
            "025252e6eb9646e9b1fdcccbd67ca151",
            "9fb4d1e9d9534f2c89b7291e7a97e67d",
            "68be814d78a643f282c1165a3c5cf459",
            "144df1faebce4618b1881494a9aaa9ea",
            "b7ed524e59bd423389e5a4d62c28bc94",
            "fbf8cde9748a49eb8eb91ee124c15111",
            "b116692ee0f747a081231491c36cebcf",
            "4f6611a3d7554ac1a9541116c46f11a9",
            "aa7d0bde14004313bad057ac008bdabb",
            "c08e526d81654590b46e7b3c19a29df5",
            "212d4745024b4951a8c8073f61383f16",
            "66ed3bb9df714a0185eb0f34a1eacf39",
            "bef5788ec0c2478d9a554b338b1ebba0",
            "3d26ffd7b84a4a09a4d7645a39176f89",
            "45d47fc9d9794e5db49506999c531131",
            "7aae27c41fe543da816483c5e5311efb",
            "9e1216844bf949a2982907f8844e359d",
            "ca047816be564ba39ea303b871f1dde9",
            "1c21381134444a14b82d57802f6f3b19",
            "426a675c29d24febb3a0c6e192fdb4b0",
            "1c2454656db94f3d9ac1decefddb7917",
            "68de5c68efae40a6836f89a5287c4898",
            "cc080380fed24cd19a97604a30651cd0",
            "193e6e07dca1404699f8e671798f50da",
            "d1d728e572d7480ca14926da338f03a8",
            "544a086f1e1a46d092e15c2a808bb1e7",
            "bbd2a95919fe40be873e3452118765c1",
            "76f3a78b877745459687ce84e88b122a",
            "124fffa333cf4b2fa0466a890a107a6b",
            "c6344f27101340ef85365788dc6e7676",
            "065f70531ec142a385e01855f48c0884",
            "213ec86c13af4ea582b473b34153cca5",
            "b25ddc2152ca46cd8dd2ce5576a4d615",
            "536122ef067f48f1bf095c338a263a14",
            "1c4ce11398a34b5292ac9d1515ccbd08",
            "442a812ac25649f2bd375f589d765d38",
            "1a5805685d064039aed6735c182dc820",
            "bad6b75ad4ed4d99aaca8d02722228cf",
            "7a322d8f894c44d6a6b39f46fbe619af",
            "df291c05d9ab4759a4825e7fb9916759",
            "44f71b499f894836b87c4ff11a60ed04",
            "38cc0736f22e4afebdfe16502063b37c",
            "cc8df79e7c8147eda3fa50a6533e3a02",
            "ca8ff0e1d10d4b4c80dc1317c8736ee5",
            "ce8d302278d64e04a89cdcd28080e991",
            "9d4a89979aff4280a39e72947ab074f8",
            "404fe47dec5449d8ab5832db5dd3dada",
            "60feaed683dc4c8a9bed466065228558",
            "2877ef40d7fa4de5ade724cbcf307546",
            "37c5dd2a090a4f9abe4d16df4ea27848",
            "85ff5060fd1f47e0aa3a9176d6bd9fc0",
            "c5a020f29c834da1a2012dfa872c2b07",
            "5da2c1cb505743b4a50366aa94cb2e63",
            "b402ea75f70d47f7876ed158c5715a7d",
            "2f463d36709d490a8c975b64707d9a92",
            "e3b2e67faa4645cf907807456f9d6742",
            "ae7e0b918a3b402094c7a6653392c288",
            "00c45d51351148ca9cb5322f2fc206bb",
            "adc7d7caa7f54772a493390e5c46aa07",
            "b3e50b69c9624575a08fbd41f3d888b6",
            "2b36429b497a4490bb8fb7366bc7f01b",
            "7e25e4e169b44840b8c5ec5ee7017136",
            "01914633928f435980af44e2a09124a8",
            "a63ad3d929424f34aab2df31603d7c3c",
            "6bdb8d4a9f284053a7e5617f1082747d"
          ]
        },
        "id": "DyFJFppSC9Rl",
        "outputId": "abb4d4dc-5900-43c0-b1d2-0300c1aaa51a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/721 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc99d0197f26458e821e463b17bfc9e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.10G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "212d4745024b4951a8c8073f61383f16"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68de5c68efae40a6836f89a5287c4898"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/1.10M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b25ddc2152ca46cd8dd2ce5576a4d615"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/3.49M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ca8ff0e1d10d4b4c80dc1317c8736ee5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f463d36709d490a8c975b64707d9a92"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"translation\", model=\"VietAI/envit5-translation\", max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2GjrBD3rJKJ",
        "outputId": "fdb53dc1-1886-45a6-de5d-23450bef8405"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vi: Căn cứ vào bối cảnh đã được cung cấp, không có thông tin về Khoa Khoa học tự nhiên, Trường Đại học Khoa học thành phố Hồ Chí Minh. Nội dung chỉ bao gồm thông tin về Khoa Khoa học Môi trường, Khoa Ngoại ngữ thuộc Khoa Khoa học Nhân văn và Xã hội, Khoa Kỹ thuật Điện của Trường Đại học Khoa học, không có thông tin chi tiết về nhóm Toán học hoặc ngành học tương ứng.\n",
            "vi: Ngành tuyển sinh nhiều nhất tại Đại học Khoa học, Đại học Quốc gia thành phố Hồ Chí Minh là chương trình Tiếng Pháp với quy mô 80 sinh viên. Thông tin này được đề cập trong ngữ cảnh mã số 7220203.\n",
            "vi: The Artificial Intelligence training course provides students with knowledge of Computer Vision. Therefore, students can learn about Computer Vision at the Department of Computer Science - University of Information Technology, ĐHQG-HCM.\n",
            "vi: Nội dung văn bản cung cấp không bao gồm thông tin về số khoa thuộc \"Đại học Kinh tế-Luật, Đại họcQuốc gia-HCM\".\n",
            "vi: Chuyên ngành Quản trị Kinh doanh (Quản trị Kinh doanh) tại Đại học Kinh tế-Luật, Đại học Quốc gia HCM có hai chuyên ngành: Quản trị Kinh doanh (Quản trị Kinh doanh) và Quản trị Du lịch và Lữ hành (Quản lý Du lịch và Vận tải).\n",
            "vi: Dựa trên bối cảnh được cung cấp, Chương trình Marketing thuộc Khoa Quản trị kinh doanh tại Đại học Kinh tế và Luật, Đại học Quốc gia thành phố Hồ Chí Minh nhằm đào tạo các chuyên gia marketing, các nhà phân tích marketing, tư vấn marketing, các nhà phát triển chính sách marketing, các nhà nghiên cứu marketing và các nhà quản lý marketing. Tuy nhiên, số lượng sinh viên cụ thể mà chương trình này sẽ nhận cho năm 2024 không được đề cập trong bối cảnh.\n",
            "vi: Căn cứ vào bối cảnh được cung cấp, khoa Hệ thống thông tin thuộc khoa Công nghệ thông tin tại Đại học Kinh tế Quốc gia, Thành phố Hồ Chí Minh tổ chức các chương trình đào tạo nâng cao về Công nghệ thông tin địa lý (GIS) và Công nghệ thông tin quản lý (MIS) với sự phối hợp của các khoa Địa lý và Hệ thống thông tin quản lý tại Đại học Oklahoma, Hoa Kỳ. Do đó, các lĩnh vực GIS và MIS có thể được coi là chuyên ngành trong lĩnh vực quản lý hệ thống thông tin tại trường đại học.\n",
            "vi: Vâng, theo thông tin đã cung cấp, Khoa Y thuộc Đại học Khoa học và Công nghệ TP. HCM có khả năng đào tạo sinh viên ngành Phòng chống thiên tai Y. Điều này được suy ra từ cam kết của khoa về việc phát triển các phòng thí nghiệm tiên tiến và mời các chuyên gia trong ngành hỗ trợ giảng dạy và nghiên cứu. Tuy nhiên, trong bối cảnh hiện nay, ngành Y không được xác định cụ thể. Nếu Y là \"Phòng chống thiên tai địa chất\" thì Khoa Địa chất thuộc Đại học Khoa học TP. HCM được biết đến với chuyên môn sâu trong lĩnh vực này.\n",
            "vi: The Department of Pedagogy, An Giang University, UHCM trains disciplines related to becoming teachers with high quality. They focus on training those who have pedagogical skills, research and innovative application of scientific products, ready to serve for teaching and learning activities. The Department is not specialized in training in some specific technological fields such as construction, food technology or quinoa.\n",
            "vi: Học phí trung bình cho các chương trình đại học toàn thời gian tại Đại học Công nghệ và Giáo dục TP.HCM năm 2024 là 32.500.000 đồng.\n",
            "vi: Chương trình chuẩn cho Khoa Kỹ thuật Dầu khí và Địa chất tại Khoa Kỹ thuật, Đại học Khoa học và Công nghệ Thành phố Hồ Chí Minh (Bạch Khoa), sẽ đào tạo 90 sinh viên trong chương trình Kỹ thuật Dầu khí và Địa chất vào năm 2024.\n",
            "vi: Các lĩnh vực nghiên cứu của Trường Du lịch thuộc Đại học Khoa học Xã hội và Nhân văn, Đại học Quốc gia Thành phố Hồ Chí Minh là Quản lý Du lịch và Hướng dẫn Du lịch. Trường Du lịch cung cấp các chuyên ngành sau: Hướng dẫn Du lịch, Quản lý Kinh doanh Du lịch và Quản lý Khách sạn (bao gồm Quản lý Nhà hàng, Khách sạn và Khu nghỉ dưỡng).\n",
            "vi: Đề cương chương trình chuẩn ngành Công nghệ thông tin tại Trường Đại học Khoa học, Đại học Quốc gia Thành phố Hồ Chí Minh chưa có mục tiêu định lượng cụ thể cho lĩnh vực Thương mại điện tử cho năm 2.\n",
            "vi: Mức thu học phí đối với các chương trình đào tạo của Trường Thương mại điện tử, Khoa Công nghệ thông tin, Đại học Quốc gia thành phố Hồ Chí Minh năm 2024 như sau: 1. Mức học phí trung bình là 32.500.000 đồng Việt Nam. 2. Mức học phí đối với chương trình đào tạo nâng cao, chất lượng cao là 80.000.000 đồng Việt Nam. 3. Mức học phí đối với chương trình đào tạo chất lượng cao với tiếng Nhật nâng cao là 60.000.000 đồng Việt Nam. Lưu ý rằng mức thu này có thể thay đổi và khuyến nghị các trường liên hệ với trường để biết thêm thông tin chính xác, cập nhật.\n",
            "vi: Vâng, Khoa Triết học có mặt tại Đại học Khoa học Xã hội và Nhân văn, Đại học Quốc gia thành phố Hồ Chí Minh. Điều này thể hiện rõ qua việc đặt tên và mô tả nhất quán các khoa trong các bộ phận khác nhau của bối cảnh được cung cấp.\n",
            "vi: Dựa trên bối cảnh được cung cấp, thông tin chi tiết về các chứng chỉ và tiêu chuẩn quốc tế mà Trường Kỹ thuật đạt được, nhưng nó không cung cấp danh sách các chương trình cụ thể do trường cung cấp. Do đó, tôi không thể cung cấp tên của các chương trình cụ thể.\n",
            "vi: Chắc chắn là tôi có thể giúp được. Dựa trên bối cảnh đã nêu, có một vài khoa hoặc trường thuộc Đại học Khoa học Xã hội và Nhân văn, Đại học Quốc gia thành phố Hồ Chí Minh có các chương trình quản lý kinh doanh. Dưới đây là một số trong số đó: 1. Khoa Quản trị Kinh doanh 2. Khoa Quản trị Kinh doanh Quốc tế 3. Khoa Marketing 4. Khoa Quản lý Khách sạn Các khoa này có các chương trình đại học và sau đại học về quản lý kinh doanh, kinh doanh quốc tế, marketing và quản lý khách sạn. Các khoa này sử dụng các giảng viên và nhà nghiên cứu có trình độ cao từ trong và ngoài nước, và hợp tác với các trường đại học uy tín trên thế giới. Cũng cần lưu ý rằng có thể có các khoa hoặc trường khác trong trường đại học có các chương trình quản lý kinh doanh, nhưng các khoa nói trên là những khoa được nêu rõ trong bối cảnh đã nêu.\n",
            "vi: Trong hệ thống các trường đại học của Đại học Quốc gia thành phố Hồ Chí Minh, Khoa Triết học thuộc Đại học Khoa học Xã hội và Nhân văn là một trong những đơn vị có chất lượng đào tạo cao nhất. Trong cùng năm 2017, Khoa Triết học đã đạt được hai tiêu chuẩn chất lượng quốc tế về giáo dục cơ bản là tiêu chuẩn HCERES của Châu Âu và tiêu chuẩn AUN-QA của mạng lưới các trường đại học ASEAN. Khoa Triết học có nhiều chương trình đào tạo chất lượng cao, tập trung vào việc bồi dưỡng tư duy phản biện, lý luận đạo đức và hiểu biết toàn diện về bản chất, xã hội và văn hóa của con người. Đào tạo ở trình độ cao, đội ngũ giảng viên mạnh đảm bảo sinh viên có sự chuẩn bị tốt cho sự nghiệp hoặc nghiên cứu sau này. Khoa Triết học đạt được hai tiêu chuẩn đào tạo, là một trong những đơn vị hàng đầu trong hệ thống các trường đại học của Đại học Quốc gia thành phố Hồ Chí Minh về chất lượng đào tạo.\n"
          ]
        }
      ],
      "source": [
        "answers = []\n",
        "contexts = []\n",
        "\n",
        "for i in questions:\n",
        "  prompt= \"Trả lời cụ thể các question bên dưới\" + i\n",
        "  response_vector = hyde_query_engine.query(prompt)\n",
        "  dic = \"en: \" + response_vector.response\n",
        "  response = pipe(dic)\n",
        "  answers.append(response[0]['translation_text'])\n",
        "  contexts.append([a.get_text() for a in response_vector.source_nodes])\n",
        "  print(response[0]['translation_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELgOCoWerKhd"
      },
      "outputs": [],
      "source": [
        "ground_truths = [[a] for a in test[\"ground_truth\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybdBMzubrQgv"
      },
      "outputs": [],
      "source": [
        "for num, i in enumerate(ground_truths):\n",
        "    if type(i) != str:\n",
        "        ground_truths[num] = str(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQ9CHA_nrSvu"
      },
      "outputs": [],
      "source": [
        "answers = [str(i) for i in answers]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1fqi_yQrVq5"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "datasample = {\n",
        "    \"question\": questions,\n",
        "    \"contexts\": contexts,\n",
        "    \"answer\": answers,\n",
        "    \"ground_truth\": ground_truths\n",
        "}\n",
        "\n",
        "dataset = Dataset.from_dict(datasample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGglqVeKrg7k"
      },
      "outputs": [],
      "source": [
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_precision,\n",
        "    context_recall,\n",
        ")\n",
        "#from ragas.metrics.critique import harmfulness\n",
        "\n",
        "metrics = [\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_precision,\n",
        "    context_recall,\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "238dff1035e44ea6a78d613da4a006dc",
            "e163dc25f5ca4fd3bdf9c9bf7762c686",
            "ddf33d695c87419c8dd2d752c3926e9b",
            "75507112021d42e594ac24f7c66aa216",
            "59ea5110c66c47bc8f65b9431a089684",
            "18d4ac2156a9484ea54d2283c9d4cddb",
            "e23ca6786a2542dfb34be284ac49563c",
            "696e3574c88b488890a376e56838d57c",
            "a3306c3a1b3e4c51bd99eefd933f50a3",
            "da1d28e563a74627b53f246917d9ac53",
            "9f674b6b06fa4ef3bf8d96141682ddd8"
          ]
        },
        "id": "dffvBk2droU2",
        "outputId": "873e4af2-ef9a-4a81-ecf0-e11b4582655d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "238dff1035e44ea6a78d613da4a006dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/72 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_answer_relevance.py\", line 152, in _ascore\n",
            "    result = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on tokens per min (TPM): Limit 60000, Used 59779, Requested 1521. Please try again in 1.3s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on tokens per min (TPM): Limit 60000, Used 58884, Requested 2218. Please try again in 1.102s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_precision.py\", line 161, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on tokens per min (TPM): Limit 60000, Used 59830, Requested 1386. Please try again in 1.216s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_precision.py\", line 161, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on tokens per min (TPM): Limit 60000, Used 59764, Requested 2241. Please try again in 2.004s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on tokens per min (TPM): Limit 60000, Used 58994, Requested 2137. Please try again in 1.131s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_faithfulness.py\", line 266, in _ascore\n",
            "    nli_result = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on tokens per min (TPM): Limit 60000, Used 59196, Requested 2021. Please try again in 1.217s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_faithfulness.py\", line 266, in _ascore\n",
            "    nli_result = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on tokens per min (TPM): Limit 60000, Used 59282, Requested 1931. Please try again in 1.213s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_faithfulness.py\", line 266, in _ascore\n",
            "    nli_result = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on tokens per min (TPM): Limit 60000, Used 59230, Requested 1888. Please try again in 1.118s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_precision.py\", line 161, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on tokens per min (TPM): Limit 60000, Used 58923, Requested 1343. Please try again in 266ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_precision.py\", line 161, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on tokens per min (TPM): Limit 60000, Used 59074, Requested 1236. Please try again in 310ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_answer_relevance.py\", line 152, in _ascore\n",
            "    result = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on tokens per min (TPM): Limit 60000, Used 59441, Requested 1801. Please try again in 1.242s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_precision.py\", line 161, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on tokens per min (TPM): Limit 60000, Used 58855, Requested 1354. Please try again in 209ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_precision.py\", line 161, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on tokens per min (TPM): Limit 60000, Used 59524, Requested 1302. Please try again in 826ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_faithfulness.py\", line 248, in _ascore\n",
            "    statements = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_precision.py\", line 161, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_precision.py\", line 161, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_precision.py\", line 161, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on tokens per min (TPM): Limit 60000, Used 59375, Requested 1292. Please try again in 667ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_faithfulness.py\", line 266, in _ascore\n",
            "    nli_result = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on tokens per min (TPM): Limit 60000, Used 59325, Requested 2360. Please try again in 1.685s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on tokens per min (TPM): Limit 60000, Used 59306, Requested 1964. Please try again in 1.27s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_precision.py\", line 161, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_faithfulness.py\", line 266, in _ascore\n",
            "    nli_result = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on tokens per min (TPM): Limit 60000, Used 58568, Requested 2306. Please try again in 874ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on tokens per min (TPM): Limit 60000, Used 59680, Requested 2026. Please try again in 1.706s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_faithfulness.py\", line 266, in _ascore\n",
            "    nli_result = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on tokens per min (TPM): Limit 60000, Used 59756, Requested 1901. Please try again in 1.657s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_precision.py\", line 161, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on tokens per min (TPM): Limit 60000, Used 59113, Requested 2112. Please try again in 1.225s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_faithfulness.py\", line 266, in _ascore\n",
            "    nli_result = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on tokens per min (TPM): Limit 60000, Used 59033, Requested 2114. Please try again in 1.147s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_faithfulness.py\", line 248, in _ascore\n",
            "    statements = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_answer_relevance.py\", line 152, in _ascore\n",
            "    result = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_answer_relevance.py\", line 152, in _ascore\n",
            "    result = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on tokens per min (TPM): Limit 60000, Used 59355, Requested 1806. Please try again in 1.161s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on tokens per min (TPM): Limit 60000, Used 59235, Requested 2025. Please try again in 1.26s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_faithfulness.py\", line 266, in _ascore\n",
            "    nli_result = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on tokens per min (TPM): Limit 60000, Used 58922, Requested 1809. Please try again in 731ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_precision.py\", line 161, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on tokens per min (TPM): Limit 60000, Used 59618, Requested 1348. Please try again in 966ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on tokens per min (TPM): Limit 60000, Used 58613, Requested 2149. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_answer_relevance.py\", line 152, in _ascore\n",
            "    result = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on tokens per min (TPM): Limit 60000, Used 59689, Requested 1862. Please try again in 1.551s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_precision.py\", line 161, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on tokens per min (TPM): Limit 60000, Used 59244, Requested 2324. Please try again in 1.568s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_answer_relevance.py\", line 152, in _ascore\n",
            "    result = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_precision.py\", line 161, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_precision.py\", line 161, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_precision.py\", line 161, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on tokens per min (TPM): Limit 60000, Used 59027, Requested 2246. Please try again in 1.273s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_faithfulness.py\", line 248, in _ascore\n",
            "    statements = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_faithfulness.py\", line 266, in _ascore\n",
            "    nli_result = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 687, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 647, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 832, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 672, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1289, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1805, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1503, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1599, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-omvEiidrFUoGpsXZGmMyUxNH on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
          ]
        }
      ],
      "source": [
        "from ragas import evaluate\n",
        "#from ragas.metrics import context_precision, faithfulness, answer_relevancy, context_recall\n",
        "\n",
        "#result = evaluate(dataset, metrics=metrics)\n",
        "\n",
        "result = evaluate(dataset, metrics=[faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_precision,\n",
        "    context_recall], raise_exceptions=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rs = result.to_pandas()\n",
        "rs.head()"
      ],
      "metadata": {
        "id": "cey7Md9m8ZhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfU3MIujtpjX"
      },
      "outputs": [],
      "source": [
        "rs.to_csv(\"test.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "18d4ac2156a9484ea54d2283c9d4cddb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "238dff1035e44ea6a78d613da4a006dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e163dc25f5ca4fd3bdf9c9bf7762c686",
              "IPY_MODEL_ddf33d695c87419c8dd2d752c3926e9b",
              "IPY_MODEL_75507112021d42e594ac24f7c66aa216"
            ],
            "layout": "IPY_MODEL_59ea5110c66c47bc8f65b9431a089684"
          }
        },
        "59ea5110c66c47bc8f65b9431a089684": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "696e3574c88b488890a376e56838d57c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75507112021d42e594ac24f7c66aa216": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da1d28e563a74627b53f246917d9ac53",
            "placeholder": "​",
            "style": "IPY_MODEL_9f674b6b06fa4ef3bf8d96141682ddd8",
            "value": " 72/72 [13:45&lt;00:00, 16.97s/it]"
          }
        },
        "9f674b6b06fa4ef3bf8d96141682ddd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3306c3a1b3e4c51bd99eefd933f50a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da1d28e563a74627b53f246917d9ac53": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddf33d695c87419c8dd2d752c3926e9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_696e3574c88b488890a376e56838d57c",
            "max": 72,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a3306c3a1b3e4c51bd99eefd933f50a3",
            "value": 72
          }
        },
        "e163dc25f5ca4fd3bdf9c9bf7762c686": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18d4ac2156a9484ea54d2283c9d4cddb",
            "placeholder": "​",
            "style": "IPY_MODEL_e23ca6786a2542dfb34be284ac49563c",
            "value": "Evaluating: 100%"
          }
        },
        "e23ca6786a2542dfb34be284ac49563c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc99d0197f26458e821e463b17bfc9e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_025252e6eb9646e9b1fdcccbd67ca151",
              "IPY_MODEL_9fb4d1e9d9534f2c89b7291e7a97e67d",
              "IPY_MODEL_68be814d78a643f282c1165a3c5cf459"
            ],
            "layout": "IPY_MODEL_144df1faebce4618b1881494a9aaa9ea"
          }
        },
        "025252e6eb9646e9b1fdcccbd67ca151": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7ed524e59bd423389e5a4d62c28bc94",
            "placeholder": "​",
            "style": "IPY_MODEL_fbf8cde9748a49eb8eb91ee124c15111",
            "value": "config.json: 100%"
          }
        },
        "9fb4d1e9d9534f2c89b7291e7a97e67d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b116692ee0f747a081231491c36cebcf",
            "max": 721,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f6611a3d7554ac1a9541116c46f11a9",
            "value": 721
          }
        },
        "68be814d78a643f282c1165a3c5cf459": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa7d0bde14004313bad057ac008bdabb",
            "placeholder": "​",
            "style": "IPY_MODEL_c08e526d81654590b46e7b3c19a29df5",
            "value": " 721/721 [00:00&lt;00:00, 43.7kB/s]"
          }
        },
        "144df1faebce4618b1881494a9aaa9ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7ed524e59bd423389e5a4d62c28bc94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbf8cde9748a49eb8eb91ee124c15111": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b116692ee0f747a081231491c36cebcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f6611a3d7554ac1a9541116c46f11a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aa7d0bde14004313bad057ac008bdabb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c08e526d81654590b46e7b3c19a29df5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "212d4745024b4951a8c8073f61383f16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_66ed3bb9df714a0185eb0f34a1eacf39",
              "IPY_MODEL_bef5788ec0c2478d9a554b338b1ebba0",
              "IPY_MODEL_3d26ffd7b84a4a09a4d7645a39176f89"
            ],
            "layout": "IPY_MODEL_45d47fc9d9794e5db49506999c531131"
          }
        },
        "66ed3bb9df714a0185eb0f34a1eacf39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7aae27c41fe543da816483c5e5311efb",
            "placeholder": "​",
            "style": "IPY_MODEL_9e1216844bf949a2982907f8844e359d",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "bef5788ec0c2478d9a554b338b1ebba0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca047816be564ba39ea303b871f1dde9",
            "max": 1100503117,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c21381134444a14b82d57802f6f3b19",
            "value": 1100503117
          }
        },
        "3d26ffd7b84a4a09a4d7645a39176f89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_426a675c29d24febb3a0c6e192fdb4b0",
            "placeholder": "​",
            "style": "IPY_MODEL_1c2454656db94f3d9ac1decefddb7917",
            "value": " 1.10G/1.10G [00:17&lt;00:00, 55.4MB/s]"
          }
        },
        "45d47fc9d9794e5db49506999c531131": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7aae27c41fe543da816483c5e5311efb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e1216844bf949a2982907f8844e359d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca047816be564ba39ea303b871f1dde9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c21381134444a14b82d57802f6f3b19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "426a675c29d24febb3a0c6e192fdb4b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c2454656db94f3d9ac1decefddb7917": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68de5c68efae40a6836f89a5287c4898": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc080380fed24cd19a97604a30651cd0",
              "IPY_MODEL_193e6e07dca1404699f8e671798f50da",
              "IPY_MODEL_d1d728e572d7480ca14926da338f03a8"
            ],
            "layout": "IPY_MODEL_544a086f1e1a46d092e15c2a808bb1e7"
          }
        },
        "cc080380fed24cd19a97604a30651cd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbd2a95919fe40be873e3452118765c1",
            "placeholder": "​",
            "style": "IPY_MODEL_76f3a78b877745459687ce84e88b122a",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "193e6e07dca1404699f8e671798f50da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_124fffa333cf4b2fa0466a890a107a6b",
            "max": 1189,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c6344f27101340ef85365788dc6e7676",
            "value": 1189
          }
        },
        "d1d728e572d7480ca14926da338f03a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_065f70531ec142a385e01855f48c0884",
            "placeholder": "​",
            "style": "IPY_MODEL_213ec86c13af4ea582b473b34153cca5",
            "value": " 1.19k/1.19k [00:00&lt;00:00, 54.8kB/s]"
          }
        },
        "544a086f1e1a46d092e15c2a808bb1e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbd2a95919fe40be873e3452118765c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76f3a78b877745459687ce84e88b122a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "124fffa333cf4b2fa0466a890a107a6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6344f27101340ef85365788dc6e7676": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "065f70531ec142a385e01855f48c0884": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "213ec86c13af4ea582b473b34153cca5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b25ddc2152ca46cd8dd2ce5576a4d615": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_536122ef067f48f1bf095c338a263a14",
              "IPY_MODEL_1c4ce11398a34b5292ac9d1515ccbd08",
              "IPY_MODEL_442a812ac25649f2bd375f589d765d38"
            ],
            "layout": "IPY_MODEL_1a5805685d064039aed6735c182dc820"
          }
        },
        "536122ef067f48f1bf095c338a263a14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bad6b75ad4ed4d99aaca8d02722228cf",
            "placeholder": "​",
            "style": "IPY_MODEL_7a322d8f894c44d6a6b39f46fbe619af",
            "value": "spiece.model: 100%"
          }
        },
        "1c4ce11398a34b5292ac9d1515ccbd08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df291c05d9ab4759a4825e7fb9916759",
            "max": 1102207,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_44f71b499f894836b87c4ff11a60ed04",
            "value": 1102207
          }
        },
        "442a812ac25649f2bd375f589d765d38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38cc0736f22e4afebdfe16502063b37c",
            "placeholder": "​",
            "style": "IPY_MODEL_cc8df79e7c8147eda3fa50a6533e3a02",
            "value": " 1.10M/1.10M [00:00&lt;00:00, 14.5MB/s]"
          }
        },
        "1a5805685d064039aed6735c182dc820": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bad6b75ad4ed4d99aaca8d02722228cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a322d8f894c44d6a6b39f46fbe619af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df291c05d9ab4759a4825e7fb9916759": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44f71b499f894836b87c4ff11a60ed04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "38cc0736f22e4afebdfe16502063b37c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc8df79e7c8147eda3fa50a6533e3a02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca8ff0e1d10d4b4c80dc1317c8736ee5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ce8d302278d64e04a89cdcd28080e991",
              "IPY_MODEL_9d4a89979aff4280a39e72947ab074f8",
              "IPY_MODEL_404fe47dec5449d8ab5832db5dd3dada"
            ],
            "layout": "IPY_MODEL_60feaed683dc4c8a9bed466065228558"
          }
        },
        "ce8d302278d64e04a89cdcd28080e991": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2877ef40d7fa4de5ade724cbcf307546",
            "placeholder": "​",
            "style": "IPY_MODEL_37c5dd2a090a4f9abe4d16df4ea27848",
            "value": "tokenizer.json: 100%"
          }
        },
        "9d4a89979aff4280a39e72947ab074f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85ff5060fd1f47e0aa3a9176d6bd9fc0",
            "max": 3488920,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c5a020f29c834da1a2012dfa872c2b07",
            "value": 3488920
          }
        },
        "404fe47dec5449d8ab5832db5dd3dada": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5da2c1cb505743b4a50366aa94cb2e63",
            "placeholder": "​",
            "style": "IPY_MODEL_b402ea75f70d47f7876ed158c5715a7d",
            "value": " 3.49M/3.49M [00:00&lt;00:00, 33.8MB/s]"
          }
        },
        "60feaed683dc4c8a9bed466065228558": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2877ef40d7fa4de5ade724cbcf307546": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37c5dd2a090a4f9abe4d16df4ea27848": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85ff5060fd1f47e0aa3a9176d6bd9fc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5a020f29c834da1a2012dfa872c2b07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5da2c1cb505743b4a50366aa94cb2e63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b402ea75f70d47f7876ed158c5715a7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f463d36709d490a8c975b64707d9a92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e3b2e67faa4645cf907807456f9d6742",
              "IPY_MODEL_ae7e0b918a3b402094c7a6653392c288",
              "IPY_MODEL_00c45d51351148ca9cb5322f2fc206bb"
            ],
            "layout": "IPY_MODEL_adc7d7caa7f54772a493390e5c46aa07"
          }
        },
        "e3b2e67faa4645cf907807456f9d6742": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3e50b69c9624575a08fbd41f3d888b6",
            "placeholder": "​",
            "style": "IPY_MODEL_2b36429b497a4490bb8fb7366bc7f01b",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "ae7e0b918a3b402094c7a6653392c288": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e25e4e169b44840b8c5ec5ee7017136",
            "max": 1109,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_01914633928f435980af44e2a09124a8",
            "value": 1109
          }
        },
        "00c45d51351148ca9cb5322f2fc206bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a63ad3d929424f34aab2df31603d7c3c",
            "placeholder": "​",
            "style": "IPY_MODEL_6bdb8d4a9f284053a7e5617f1082747d",
            "value": " 1.11k/1.11k [00:00&lt;00:00, 59.4kB/s]"
          }
        },
        "adc7d7caa7f54772a493390e5c46aa07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3e50b69c9624575a08fbd41f3d888b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b36429b497a4490bb8fb7366bc7f01b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e25e4e169b44840b8c5ec5ee7017136": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01914633928f435980af44e2a09124a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a63ad3d929424f34aab2df31603d7c3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bdb8d4a9f284053a7e5617f1082747d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}